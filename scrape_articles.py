import requests
from bs4 import BeautifulSoup
import json
import re

def get_main_page():
    """è·å–ä¸»é¡µé¢å†…å®¹"""
    url = "https://edu.cnr.cn/eduzt/ywkwsfsd/"
    try:
        response = requests.get(url, timeout=10)
        response.encoding = 'gb2312'
        return response.text
    except Exception as e:
        print(f"è·å–ä¸»é¡µé¢å¤±è´¥: {e}")
        return None

def parse_articles(html):
    """è§£æä¸»é¡µé¢ä¸­çš„æ–‡ç« ä¿¡æ¯"""
    soup = BeautifulSoup(html, 'html.parser')
    articles = []
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡ç« ä¿¡æ¯çš„å®¹å™¨
    containers = soup.find_all('div', class_='container')
    
    for container in containers:
        try:
            # è·å–æ–‡ç« é“¾æ¥
            link_tag = container.find('a')
            if not link_tag or 'href' not in link_tag.attrs:
                continue
                
            href = link_tag['href']
            if not href.startswith('http'):
                href = 'https:' + href if href.startswith('//') else 'https://www.cnr.cn' + href
            
            # è·å–è¯¾ç¨‹ç¼–å·
            book_span = container.find('div', class_='book').find('span')
            lesson_number = book_span.get_text(strip=True) if book_span else ""
            
            # è·å–æ ‡é¢˜å’Œæœ—è¯»è€…
            book_intro = container.find('div', class_='bookIntro')
            if book_intro:
                title_p = book_intro.find('p', class_='f18')
                reader_p = book_intro.find('p', class_='f16')
                
                title = title_p.get_text(strip=True) if title_p else ""
                reader = reader_p.get_text(strip=True) if reader_p else ""
                
                # æ¸…ç†è¯»è€…ä¿¡æ¯
                if "æœ—   è¯»    è€…ï¼š" in reader:
                    reader = reader.replace("æœ—   è¯»    è€…ï¼š", "")
            
            articles.append({
                'detail_url': href,
                'lesson_number': lesson_number,
                'title': title,
                'reader': reader
            })
            
        except Exception as e:
            print(f"è§£ææ–‡ç« ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            continue
    
    return articles

def get_article_detail(url):
    """è·å–æ–‡ç« è¯¦æƒ…é¡µå†…å®¹ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    max_retries = 3
    retry_delay = 2  # ç§’
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            
            # æ£€æŸ¥æ˜¯å¦ä¸º502é”™è¯¯
            if response.status_code == 502:
                print(f"502é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {url}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    return None
            
            response.encoding = 'gb2312'
            return response.text
            
        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}) {url}: {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(retry_delay * (attempt + 1))
                continue
            else:
                return None
        except Exception as e:
            print(f"å…¶ä»–é”™è¯¯ {url}: {e}")
            return None
    
    return None

def parse_article_detail(html, base_info):
    """è§£ææ–‡ç« è¯¦æƒ…é¡µå†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_tag = soup.find('h1')
    content_title = title_tag.get_text(strip=True) if title_tag else base_info['title']
    
    # è·å–ä½œè€…ä¿¡æ¯ - ä»metaæ ‡ç­¾è·å–
    author = ""
    author_meta = soup.find('meta', {'name': 'author'})
    if author_meta and 'content' in author_meta.attrs:
        author = author_meta['content'].strip()
    
    # è·å–æœ—è¯»è€…ä¿¡æ¯
    reader = ""
    source_div = soup.find('div', class_='source')
    if source_div:
        source_text = source_div.get_text(strip=True)
        if "æœ—è¯»è€…:" in source_text:
            reader = source_text.replace("æœ—è¯»è€…:", "").strip()
    
    # è·å–éŸ³é¢‘URL
    audio_url = ""
    audio_div = soup.find('div', class_='HiRadioPlayer')
    if audio_div and 'data-url' in audio_div.attrs:
        audio_url = audio_div['data-url']
        if audio_url and not audio_url.startswith('http'):
            audio_url = 'https:' + audio_url if audio_url.startswith('//') else audio_url
    
    # è·å–æ–‡ç« å†…å®¹
    content = ""
    content_div = soup.find('div', class_='article-content')
    if content_div:
        # æ‰¾åˆ°<!--content-->æ³¨é‡Šåçš„å†…å®¹
        comments = content_div.find_all(string=lambda text: isinstance(text, str) and 'content' in text)
        if comments:
            # è·å–æ³¨é‡Šåçš„æ‰€æœ‰æ®µè½
            paragraphs = content_div.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•å…¶ä»–æ–¹å¼
    if not content:
        main_div = soup.find('div', class_='article-main')
        if main_div:
            paragraphs = main_div.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
    
    result = {
        'lesson_number': base_info['lesson_number'],
        'title': content_title,
        'author': author,  # å¯èƒ½ä¸ºç©º
        'reader': reader,  # ä»è¯¦æƒ…é¡µè·å–æœ—è¯»è€…
        'content': content,
        'audio_url': audio_url,
        'detail_url': base_info['detail_url']
    }
    
    return result

def main():
    print("å¼€å§‹æŠ“å–æ–‡ç« æ•°æ®...")
    
    # è·å–ä¸»é¡µé¢
    main_html = get_main_page()
    if not main_html:
        print("æ— æ³•è·å–ä¸»é¡µé¢")
        return
    
    # è§£ææ–‡ç« åˆ—è¡¨
    articles = parse_articles(main_html)
    print(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    
    # å°è¯•åŠ è½½ç°æœ‰æ•°æ®ä»¥ç»§ç»­å¤„ç†
    all_articles_data = []
    start_index = 0
    try:
        with open('all_articles_data.json', 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
            if existing_data:
                all_articles_data = existing_data
                start_index = len(existing_data)
                print(f"å‘ç°å·²æœ‰ {start_index} ç¯‡æ–‡ç« æ•°æ®ï¼Œä»ç¬¬ {start_index + 1} ç¯‡ç»§ç»­å¤„ç†")
    except (FileNotFoundError, json.JSONDecodeError):
        print("æœªå‘ç°ç°æœ‰æ•°æ®ï¼Œä»å¤´å¼€å§‹å¤„ç†")
        all_articles_data = []
        start_index = 0
    
    # å¤„ç†æ‰€æœ‰æ–‡ç« 
    total_articles = len(articles)
    for i, article in enumerate(articles[start_index:], start=start_index):
        print(f"\n[{i+1}/{total_articles}] å¤„ç†æ–‡ç« : {article['title']}")
        
        try:
            # è·å–è¯¦æƒ…é¡µ
            detail_html = get_article_detail(article['detail_url'])
            if detail_html:
                # è§£æè¯¦æƒ…é¡µ
                article_data = parse_article_detail(detail_html, article)
                all_articles_data.append(article_data)
                print(f"âœ“ æˆåŠŸè·å–: {article_data['title']}")
            else:
                # ä¿å­˜502é”™è¯¯çš„æ–‡ç« ä¿¡æ¯ç”¨äºäººå·¥æ ¡å¯¹
                failed_article = {
                    'lesson_number': article['lesson_number'],
                    'title': article['title'],
                    'author': "",
                    'reader': article.get('reader', ""),
                    'content': "",
                    'audio_url': "",
                    'detail_url': article['detail_url'],
                    'error': "502 Bad Gateway"
                }
                all_articles_data.append(failed_article)
                print(f"âœ— 502é”™è¯¯æ–‡ç« å·²ä¿å­˜: {article['title']}")
        except Exception as e:
            print(f"âœ— å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {article['title']}, é”™è¯¯: {e}")
        
        # æ¯å¤„ç†5ç¯‡æ–‡ç« ä¿å­˜ä¸€æ¬¡è¿›åº¦
        if (i + 1) % 5 == 0:
            print(f"ğŸ’¾ ä¿å­˜è¿›åº¦ ({i+1}/{total_articles})...")
            with open('all_articles_data.json', 'w', encoding='utf-8') as f:
                json.dump(all_articles_data, f, ensure_ascii=False, indent=2)
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        import time
        time.sleep(0.5)  # å‡å°‘å»¶è¿Ÿä»¥åŠ å¿«é€Ÿåº¦
    
    # ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ–‡ä»¶
    if all_articles_data:
        with open('all_articles_data.json', 'w', encoding='utf-8') as f:
            json.dump(all_articles_data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æˆåŠŸæŠ“å– {len(all_articles_data)} ç¯‡æ–‡ç« ï¼Œæ•°æ®å·²ä¿å­˜åˆ° all_articles_data.json")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"- æ€»æ–‡ç« æ•°: {len(all_articles_data)}")
        print(f"- æœ‰ä½œè€…ä¿¡æ¯çš„æ–‡ç« : {len([a for a in all_articles_data if a['author']])}")
        print(f"- æœ‰éŸ³é¢‘é“¾æ¥çš„æ–‡ç« : {len([a for a in all_articles_data if a['audio_url']])}")
        
        # æ˜¾ç¤ºå‰å‡ ç¯‡æ–‡ç« çš„æ ‡é¢˜ä½œä¸ºé¢„è§ˆ
        print(f"\nğŸ“‹ å‰5ç¯‡æ–‡ç« é¢„è§ˆ:")
        for i, article in enumerate(all_articles_data[:5]):
            print(f"  {i+1}. {article['title']} (ç¬¬{article['lesson_number']}è¯¾)")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæŠ“å–ä»»ä½•æ–‡ç« æ•°æ®")

if __name__ == "__main__":
    main()