import requests
from bs4 import BeautifulSoup
import json
import time

def get_article_detail(url):
    """è·å–æ–‡ç« è¯¦æƒ…é¡µå†…å®¹ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    max_retries = 3
    retry_delay = 2  # ç§’
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            
            # æ£€æŸ¥æ˜¯å¦ä¸º502é”™è¯¯
            if response.status_code == 502:
                print(f"502é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {url}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    return None
            
            response.encoding = 'gb2312'
            return response.text
            
        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}) {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (attempt + 1))
                continue
            else:
                return None
        except Exception as e:
            print(f"å…¶ä»–é”™è¯¯ {url}: {e}")
            return None
    
    return None

def parse_article_detail(html, base_info):
    """è§£ææ–‡ç« è¯¦æƒ…é¡µå†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_tag = soup.find('h1')
    content_title = title_tag.get_text(strip=True) if title_tag else base_info['title']
    
    # è·å–ä½œè€…ä¿¡æ¯ - ä»metaæ ‡ç­¾è·å–
    author = ""
    author_meta = soup.find('meta', {'name': 'author'})
    if author_meta and 'content' in author_meta.attrs:
        author = author_meta['content'].strip()
    
    # è·å–æœ—è¯»è€…ä¿¡æ¯
    reader = ""
    source_div = soup.find('div', class_='source')
    if source_div:
        source_text = source_div.get_text(strip=True)
        if "æœ—è¯»è€…:" in source_text:
            reader = source_text.replace("æœ—è¯»è€…:", "").strip()
    
    # è·å–éŸ³é¢‘URL
    audio_url = ""
    audio_div = soup.find('div', class_='HiRadioPlayer')
    if audio_div and 'data-url' in audio_div.attrs:
        audio_url = audio_div['data-url']
        if audio_url and not audio_url.startswith('http'):
            audio_url = 'https:' + audio_url if audio_url.startswith('//') else audio_url
    
    # è·å–æ–‡ç« å†…å®¹
    content = ""
    content_div = soup.find('div', class_='article-content')
    if content_div:
        # æ‰¾åˆ°<!--content-->æ³¨é‡Šåçš„å†…å®¹
        comments = content_div.find_all(string=lambda text: isinstance(text, str) and 'content' in text)
        if comments:
            # è·å–æ³¨é‡Šåçš„æ‰€æœ‰æ®µè½
            paragraphs = content_div.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•å…¶ä»–æ–¹å¼
    if not content:
        main_div = soup.find('div', class_='article-main')
        if main_div:
            paragraphs = main_div.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
    
    result = {
        'lesson_number': base_info['lesson_number'],
        'title': content_title,
        'author': author,  # å¯èƒ½ä¸ºç©º
        'reader': reader,  # ä»è¯¦æƒ…é¡µè·å–æœ—è¯»è€…
        'content': content,
        'audio_url': audio_url,
        'detail_url': base_info['detail_url']
    }
    
    return result

def get_article_info_from_main_page(url):
    """ä»ä¸»é¡µé¢è·å–æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯"""
    try:
        response = requests.get('https://edu.cnr.cn/eduzt/ywkwsfsd/', timeout=10)
        response.encoding = 'gb2312'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        containers = soup.find_all('div', class_='container')
        for container in containers:
            link_tag = container.find('a')
            if link_tag and 'href' in link_tag.attrs:
                href = link_tag['href']
                if not href.startswith('http'):
                    href = 'https:' + href if href.startswith('//') else 'https://www.cnr.cn' + href
                
                if href == url:
                    # è·å–è¯¾ç¨‹ç¼–å·
                    book_span = container.find('div', class_='book').find('span')
                    lesson_number = book_span.get_text(strip=True) if book_span else ""
                    
                    # è·å–æ ‡é¢˜å’Œæœ—è¯»è€…
                    book_intro = container.find('div', class_='bookIntro')
                    if book_intro:
                        title_p = book_intro.find('p', class_='f18')
                        reader_p = book_intro.find('p', class_='f16')
                        
                        title = title_p.get_text(strip=True) if title_p else ""
                        reader = reader_p.get_text(strip=True) if reader_p else ""
                        
                        # æ¸…ç†è¯»è€…ä¿¡æ¯
                        if "æœ—   è¯»    è€…ï¼š" in reader:
                            reader = reader.replace("æœ—   è¯»    è€…ï¼š", "")
                    
                    return {
                        'detail_url': href,
                        'lesson_number': lesson_number,
                        'title': title,
                        'reader': reader
                    }
        
    except Exception as e:
        print(f"è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
    
    return None

def main():
    print("å¼€å§‹å¤„ç†ç¼ºå¤±çš„æ–‡ç« ...")
    
    # åŠ è½½ç°æœ‰æ•°æ®
    try:
        with open('all_articles_data.json', 'r', encoding='utf-8') as f:
            all_articles_data = json.load(f)
        print(f"ç°æœ‰æ–‡ç« æ•°é‡: {len(all_articles_data)}")
    except (FileNotFoundError, json.JSONDecodeError):
        print("æœªå‘ç°ç°æœ‰æ•°æ®")
        return
    
    # è·å–æ‰€æœ‰æ–‡ç« çš„URL
    existing_urls = {article['detail_url'] for article in all_articles_data if 'detail_url' in article}
    
    # è·å–ä¸»é¡µé¢æ‰€æœ‰æ–‡ç« URL
    main_page_urls = []
    try:
        response = requests.get('https://edu.cnr.cn/eduzt/ywkwsfsd/', timeout=10)
        response.encoding = 'gb2312'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        containers = soup.find_all('div', class_='container')
        for container in containers:
            link_tag = container.find('a')
            if link_tag and 'href' in link_tag.attrs:
                href = link_tag['href']
                if not href.startswith('http'):
                    href = 'https:' + href if href.startswith('//') else 'https://www.cnr.cn' + href
                main_page_urls.append(href)
        
        print(f"ä¸»é¡µé¢æ–‡ç« æ€»æ•°: {len(main_page_urls)}")
        
    except Exception as e:
        print(f"è·å–ä¸»é¡µé¢æ–‡ç« å¤±è´¥: {e}")
        return
    
    # æ‰¾å‡ºç¼ºå¤±çš„æ–‡ç« 
    missing_urls = [url for url in main_page_urls if url not in existing_urls]
    print(f"ç¼ºå¤±æ–‡ç« æ•°é‡: {len(missing_urls)}")
    
    if not missing_urls:
        print("æ²¡æœ‰ç¼ºå¤±çš„æ–‡ç« ")
        return
    
    # å¤„ç†ç¼ºå¤±çš„æ–‡ç« 
    for i, url in enumerate(missing_urls):
        print(f"\n[{i+1}/{len(missing_urls)}] å¤„ç†ç¼ºå¤±æ–‡ç« : {url}")
        
        try:
            # è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯
            base_info = get_article_info_from_main_page(url)
            if not base_info:
                print(f"âœ— æ— æ³•è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯: {url}")
                continue
            
            # è·å–è¯¦æƒ…é¡µ
            detail_html = get_article_detail(url)
            if detail_html:
                # è§£æè¯¦æƒ…é¡µ
                article_data = parse_article_detail(detail_html, base_info)
                all_articles_data.append(article_data)
                print(f"âœ“ æˆåŠŸè·å–: {article_data['title']}")
            else:
                # ä¿å­˜502é”™è¯¯çš„æ–‡ç« ä¿¡æ¯ç”¨äºäººå·¥æ ¡å¯¹
                failed_article = {
                    'lesson_number': base_info['lesson_number'],
                    'title': base_info['title'],
                    'author': "",
                    'reader': base_info.get('reader', ""),
                    'content': "",
                    'audio_url': "",
                    'detail_url': url,
                    'error': "502 Bad Gateway"
                }
                all_articles_data.append(failed_article)
                print(f"âœ— 502é”™è¯¯æ–‡ç« å·²ä¿å­˜: {base_info['title']}")
        except Exception as e:
            print(f"âœ— å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {url}, é”™è¯¯: {e}")
        
        # æ¯å¤„ç†5ç¯‡æ–‡ç« ä¿å­˜ä¸€æ¬¡è¿›åº¦
        if (i + 1) % 5 == 0:
            print(f"ğŸ’¾ ä¿å­˜è¿›åº¦ ({i+1}/{len(missing_urls)})...")
            with open('all_articles_data.json', 'w', encoding='utf-8') as f:
                json.dump(all_articles_data, f, ensure_ascii=False, indent=2)
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(1)
    
    # ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ–‡ä»¶
    if all_articles_data:
        with open('all_articles_data.json', 'w', encoding='utf-8') as f:
            json.dump(all_articles_data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æˆåŠŸå¤„ç†æ‰€æœ‰æ–‡ç« ï¼Œæ€»æ–‡ç« æ•°: {len(all_articles_data)}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"- æ€»æ–‡ç« æ•°: {len(all_articles_data)}")
        print(f"- æœ‰ä½œè€…ä¿¡æ¯çš„æ–‡ç« : {len([a for a in all_articles_data if a['author']])}")
        print(f"- æœ‰éŸ³é¢‘é“¾æ¥çš„æ–‡ç« : {len([a for a in all_articles_data if a['audio_url']])}")
        print(f"- 502é”™è¯¯æ–‡ç« : {len([a for a in all_articles_data if 'error' in a])}")
    
if __name__ == "__main__":
    main()